@inproceedings{guo-etal-2020-benchmarking,
    title = "Benchmarking of Transformer-Based Pre-Trained Models on Social Media Text Classification Datasets",
    author = "Guo, Yuting  and
      Dong, Xiangjue  and
      Al-Garadi, Mohammed Ali  and
      Sarker, Abeed  and
      Paris, Cecile  and
      Aliod, Diego Moll{\'a}",
    editor = "Kim, Maria  and
      Beck, Daniel  and
      Mistica, Meladel",
    booktitle = "Proceedings of the 18th Annual Workshop of the Australasian Language Technology Association",
    month = dec,
    year = "2020",
    address = "Virtual Workshop",
    publisher = "Australasian Language Technology Association",
    url = "https://aclanthology.org/2020.alta-1.10",
    pages = "86--91",
    abstract = "Free text data from social media is now widely used in natural language processing research, and one of the most common machine learning tasks performed on this data is classification. Generally speaking, performances of supervised classification algorithms on social media datasets are lower than those on texts from other sources, but recently-proposed transformer-based models have considerably improved upon legacy state-of-the-art systems. Currently, there is no study that compares the performances of different variants of transformer-based models on a wide range of social media text classification datasets. In this paper, we benchmark the performances of transformer-based pre-trained models on 25 social media text classification datasets, 6 of which are health-related. We compare three pre-trained language models, RoBERTa-base, BERTweet and ClinicalBioBERT in terms of classification accuracy. Our experiments show that RoBERTa-base and BERTweet perform comparably on most datasets, and considerably better than ClinicalBioBERT, even on health-related datasets.",
}


@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@inproceedings{nguyen-etal-2020-bertweet,
    title = "{BERT}weet: A pre-trained language model for {E}nglish Tweets",
    author = "Nguyen, Dat Quoc  and
      Vu, Thanh  and
      Tuan Nguyen, Anh",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.2",
    doi = "10.18653/v1/2020.emnlp-demos.2",
    pages = "9--14",
    abstract = "We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at \url{https://github.com/VinAIResearch/BERTweet}",
}

@article{chawla2002smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002}
}

@book{breiman2017classification,
  title={Classification and regression trees},
  author={Breiman, Leo},
  year={2017},
  publisher={Routledge}
}

@article{datta2018local,
  title={Local calibration of verbal autopsy algorithms},
  author={Datta, Abhirup and Fiksel, Jacob and Amouzou, Agbessi and Zeger, Scott},
  journal={arXiv preprint arXiv:1810.10572},
  year={2018}
}